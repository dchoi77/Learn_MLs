{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Pattern recognition and Machine learning (Bishop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's linear discriminant\n",
    "\n",
    "A __discriminant__ is a function that takes an instance and assigns it to a class label.\n",
    "\n",
    "For simplicity, consider a binary classification.\n",
    "\n",
    "* Dataset: $\\{ \\mathbf{x}_1,\\ldots, \\mathbf{x}_N \\}$\n",
    "* Classes: $C_1, C_2$,\n",
    "* Object:\n",
    "\n",
    "    1. Project the dataset to $\\mathbb{R}$ by $y = \\mathbf{w}^T\\mathbf{x}$ for some vector $\\mathbf{w}$.\n",
    "    1. Classify $y \\geq -w_0$ as class $C_1$ and otherwise class $C_2$ for some threshold $w_0$.\n",
    "    \n",
    "    \n",
    "Idea:\n",
    "\n",
    "* Try to maximize the difference of the projected class means $m_i = \\mathbf{w}^T\\mathbf{m}_i, i=1,2$, where $\\mathbf{m}_i = \\frac{1}{|C_i|}\\sum_{n\\in C_i} \\mathbf{x}_n$.\n",
    "    \n",
    "* Try to minimize the within-class variance for the whole date defined by $s_1^2 + s_2^2$, where $s_i^2 = \\sum_{n\\in C_i} (\\mathbf{w}^T\\mathbf{x}_n - m_i)^2$.\n",
    "\n",
    "\n",
    "Find $\\mathbf{w}$ maximizing the Fisher criterion $J(\\mathbf{w}) = \\displaystyle{\\frac{(m_1 - m_2)^2}{s_1^2 + s_2^2}}$.\n",
    "\n",
    "\n",
    "After finding $\\mathbf{w}$, how do we decide a threshold $y_0$? We may assume that the projected instances $y_n = \\mathbf{w}^T\\mathbf{x}_n$ follow a Gaussian distribution in each class. Then find the parameters of the Gaussian distributions and set $y_0$ as intersection of the Gaussian approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probalistic generative models & Quadratic discriminant\n",
    "\n",
    "From the following multiclass generalization of the logistic sigmoid\n",
    "\\begin{align*}p(C_k|\\mathbf{x}) &= \\displaystyle{\\frac{p(\\mathbf{x}|C_k)p(C_k)}{\\sum_j p(\\mathbf{x}|C_j)p(C_j)}}\\\\\n",
    "&= \\frac{e^{a_k}}{\\sum_j e^{a_j}},\n",
    "\\end{align*}\n",
    "we have $$a_k = \\ln\\left[p(\\mathbf{x}|C_k)p(C_k)\\right]$$. \n",
    "\n",
    "\n",
    "Assume that we have continuous data and $p(\\mathbf{x}|C_k) = N(\\boldsymbol{\\mu}_k, \\Sigma)$, Gaussian distributions sharing the same covariance matrix.\n",
    "\n",
    "Letting $a_k = \\mathbf{w}_k^T\\mathbf{x} + w_{k0}$, we can show \n",
    "\n",
    "* $\\mathbf{w}_k = \\Sigma^{-1} \\boldsymbol{\\mu}_k$\n",
    "\n",
    "* $w_{k0} = -\\frac{1}{2}\\boldsymbol{\\mu}_k^T \\Sigma^{-1} \\boldsymbol{\\mu}_k + \\ln p(C_k)$\n",
    "\n",
    "Note that $a_k$ is of the form $\\boldsymbol{\\mu}_k^T \\Sigma^{-1} \\mathbf{x} + c_k$, a linear function of $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "If we allow $p(\\mathbf{x}|C_k)$ to have its own covariance matrix $\\Sigma_k$, then $a_k$ are not linear functions but quadratic functions of $\\mathbf{x}$, giving rise to a __quadratic discriminant__.\n",
    "\n",
    "\n",
    "Now to estimate the prior probabilities $p(C_k)$ and the parameters $\\boldsymbol{\\mu}_k, \\Sigma$ of the Gaussian distributions, we will find maximum likelihood solutions.\n",
    "\n",
    "For simplicity, assume the case of two classes. \n",
    "\n",
    "Let $\\pi = p(C_1)$ (so $p(C_2) = 1-\\pi$) and suppose that we have a data set $(\\mathbf{x}_n, t_n)$ for $n=1,\\ldots,N$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
