{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Pattern recognition and Machine learning (Bishop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's linear discriminant\n",
    "\n",
    "A __discriminant__ is a function that takes an instance and assigns it to a class label.\n",
    "\n",
    "For simplicity, consider a binary classification.\n",
    "\n",
    "* Dataset: $\\{ \\mathbf{x}_1,\\ldots, \\mathbf{x}_N \\}$\n",
    "* Classes: $C_1, C_2$,\n",
    "* Object:\n",
    "\n",
    "    1. Project the dataset to $\\mathbb{R}$ by $y = \\mathbf{w}^T\\mathbf{x}$ for some vector $\\mathbf{w}$.\n",
    "    1. Classify $y \\geq -w_0$ as class $C_1$ and otherwise class $C_2$ for some threshold $w_0$.\n",
    "    \n",
    "    \n",
    "Idea:\n",
    "\n",
    "* Try to maximize the difference of the projected class means $m_i = \\mathbf{w}^T\\mathbf{m}_i, i=1,2$, where $\\mathbf{m}_i = \\frac{1}{|C_i|}\\sum_{n\\in C_i} \\mathbf{x}_n$.\n",
    "    \n",
    "* Try to minimize the within-class variance for the whole date defined by $s_1^2 + s_2^2$, where $s_i^2 = \\sum_{n\\in C_i} (\\mathbf{w}^T\\mathbf{x}_n - m_i)^2$.\n",
    "\n",
    "\n",
    "Find $\\mathbf{w}$ maximizing the Fisher criterion $J(\\mathbf{w}) = \\displaystyle{\\frac{(m_1 - m_2)^2}{s_1^2 + s_2^2}}$.\n",
    "\n",
    "\n",
    "After finding $\\mathbf{w}$, how do we decide a threshold $y_0$? We may assume that the projected instances $y_n = \\mathbf{w}^T\\mathbf{x}_n$ follow a Gaussian distribution in each class. Then find the parameters of the Gaussian distributions and set $y_0$ as intersection of the Gaussian approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probalistic generative models & Quadratic discriminant\n",
    "\n",
    "From the following multiclass generalization of the logistic sigmoid\n",
    "$$ p(C_k|\\mathbf{x}) = \\displaystyle{\\frac{p(\\mathbf{x}|C_k)p(C_k)}{\\sum_j p(\\mathbf{x}|C_j)p(C_j)}} = \\frac{e^{a_k}}{\\sum_j e^{a_j}},$$\n",
    "\n",
    "we have $$a_k = \\ln\\left[p(\\mathbf{x}|C_k)p(C_k)\\right]$$. \n",
    "\n",
    "\n",
    "Assume that we have continuous data and $p(\\mathbf{x}|C_k) = N(\\boldsymbol{\\mu}_k, \\Sigma)$, Gaussian distributions sharing the same covariance matrix.\n",
    "\n",
    "Letting $a_k = \\mathbf{w}_k^T\\mathbf{x} + w_{k0}$, we can show \n",
    "\n",
    "* $\\mathbf{w}_k = \\Sigma^{-1} \\boldsymbol{\\mu}_k$\n",
    "\n",
    "* $w_{k0} = -\\frac{1}{2}\\boldsymbol{\\mu}_k^T \\Sigma^{-1} \\boldsymbol{\\mu}_k + \\ln p(C_k)$\n",
    "\n",
    "Note that $a_k$ is of the form $\\boldsymbol{\\mu}_k^T \\Sigma^{-1} \\mathbf{x} + c_k$, a linear function of $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "If we allow $p(\\mathbf{x}|C_k)$ to have its own covariance matrix $\\Sigma_k$, then $a_k$ are not linear functions but quadratic functions of $\\mathbf{x}$, giving rise to a __quadratic discriminant__.\n",
    "\n",
    "\n",
    "Now to estimate the prior probabilities $p(C_k)$ and the parameters $\\boldsymbol{\\mu}_k, \\Sigma$ of the Gaussian distributions, we will find maximum likelihood solutions.\n",
    "\n",
    "For simplicity, assume the case of two classes. \n",
    "\n",
    "Let $\\pi = p(C_1)$ (so $p(C_2) = 1-\\pi$) and suppose that we have a data set $(\\mathbf{x}_n, t_n)$ for $n=1,\\ldots,N$, where $t_n = 1$ if $\\mathbf{x}_n$ is in class $C_1$ and $t_n = 0$ if $\\mathbf{x}_n$ is in class $C_2$.\n",
    "\n",
    "Then the likelihood function is \n",
    "$$\\prod_{n=1}^N \\left[ \\pi N(\\mathbf{x}_n | \\boldsymbol{\\mu}_1,\\Sigma)\\right]^{t_n} \\left[ (1-\\pi) N(\\mathbf{x}_n | \\boldsymbol{\\mu}_2,\\Sigma)\\right]^{1-t_n}$$, where $\\mathbf{t} = (t_1,\\ldots,t_N)^T$.\n",
    "\n",
    "* Maximizing the log of the likelihood function _w.r.t._ $\\pi$, we get $\\pi = |C_1|/N$.\n",
    "\n",
    "* Maximizing the log of the likelihood function _w.r.t._ $\\boldsymbol{\\mu}_k$, we find that $\\boldsymbol{\\mu}_k$ is the mean of all instances belong to class $C_k$. \n",
    "\n",
    "* Maximizing the log of the likelihood function _w.r.t._ $\\Sigma$, we get $\\Sigma = \\sum_{k=1}^2 \\frac{|C_k|}{N} \\mathbf{S}_k$, where $\\mathbf{S}_k = \\frac{1}{|C_k|} \\sum_{n\\in C_k} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "```\n",
    "\n",
    "__Linear Discriminant Analysis__: A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method.\n",
    "\n",
    "\n",
    "__Quadratic Discriminant Analysis__: A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
