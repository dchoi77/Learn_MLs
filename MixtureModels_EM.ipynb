{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Pattern Recognition and Machine Learning (Bishop)\n",
    "\n",
    "\n",
    "# K-means Clustering\n",
    "\n",
    "Let $K$ be the number of clusters in the dataset $\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\}$. \n",
    "\n",
    "For each $n=1,\\ldots,N$ and $k=1,\\ldots,K$, we introduce indicator variables $z_{nk}$ such that $z_{nk} = 1$ if $\\mathbf{x}_n$ is in cluster $k$ and $z_{nk} = 0$ otherwise.\n",
    "\n",
    "Goal: Find $z_{nk}$ and centroids $\\boldsymbol{\\mu}_k$ minimizing the object function $$J = \\sum_{n,k} z_{nk} ||\\mathbf{x}_n - \\boldsymbol{\\mu}_k||^2$$\n",
    "\n",
    "1. Initialize $\\boldsymbol{\\mu}_k$.\n",
    "1. Repeat until convergence:\n",
    "\n",
    "    1. Solve $\\partial_{z_{nk}}J = 0$ and Update $z_{nk}$; $z_{nk} = 1$ if $\\boldsymbol{\\mu}_k$ is the closest centroid to $\\mathbf{x}_n$ and $z_{nk}=0$ otherwise.\n",
    "        \n",
    "    1. Solve $\\partial_{\\boldsymbol{\\mu}_k} J = 0$ and Update $\\boldsymbol{\\mu}_k$; $\\boldsymbol{\\mu}_k = \\sum_n z_{nk} \\mathbf{x}_n / \\sum_n z_{nk}$, the mean of all instances assigned to cluster $k$.\n",
    "    \n",
    "\n",
    "Note:\n",
    "\n",
    "* We introduced discrete latent variables $z_{nk}$ in the problem.\n",
    "* Updating $z_{nk}$ and $\\boldsymbol{\\mu}_k$ correspond to the __Expectation__ and __Maximization__ steps in the EM algorithm, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixtures\n",
    "\n",
    "Let $C_k$ be the $k$th cluster with centroid $\\boldsymbol{\\mu}_k$. \n",
    "\n",
    "Assume $p(\\mathbf{x} | C_k) = N(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ and let $\\pi_k = p(C_k)$ for each $k$. \n",
    "\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K p(C_k) p(\\mathbf{x} | C_k) = \\sum_{k=1}^{K}\\pi_k N(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n",
    "\n",
    "\n",
    "As in the K-means clustering, we introduce a random variable $\\mathbf{z}=(z_1,\\ldots,z_K)$ in which \n",
    "\n",
    "* $z_k=1$ for some $k$ and all other components are 0,\n",
    "* $p(z_k=1) = \\pi_k, k=1,\\ldots,K$; $p(z_k=1)$ means $p(\\mathbf{z})=1$ where $z_k=1$ and $z_j=0$ for $\\forall j\\neq k$.\n",
    "\n",
    "Note:\n",
    "\n",
    "* $p(\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k}$\n",
    "\n",
    "* $p(\\mathbf{x}|\\mathbf{z}) = \\prod_{k=1}^K p(\\mathbf{x}|z_k=1) = \\prod_{k=1}^K N(\\mathbf{x}|\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)^{z_k}$\n",
    "\n",
    "* $p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z}) = \\sum_{k=1}^K p(z_k=1) p(\\mathbf{x}|z_k=1) =  \\sum_{k=1}^{K}\\pi_k N(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$\n",
    "\n",
    "Using the above distributions, we can find\n",
    "\n",
    "* $p(\\mathbf{x},\\mathbf{z}) = p(\\mathbf{z}) p(\\mathbf{x}|\\mathbf{z})$\n",
    "\n",
    "* $p(z_k=1 | \\mathbf{x}) = \\displaystyle{\\frac{p(z_k=1) p(\\mathbf{x}|z_k=1)}{p(\\mathbf{x})} = \\frac{\\pi_k N(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{p(\\mathbf{x})}}$, which will be denoted by $\\gamma(z_k)$.\n",
    "\n",
    "To depict samples from $p(\\mathbf{x},\\mathbf{z})$,\n",
    "\n",
    "1. generate a value $\\hat{\\mathbf{z}}$ for $\\mathbf{z}$ from the distribution $\\{\\pi_1,\\ldots,\\pi_K\\}$ and\n",
    "1. generate a value for $\\hat{\\mathbf{x}}$ from the distribution $p(\\mathbf{x}|\\hat{\\mathbf{z}})$.\n",
    "\n",
    "We can also \n",
    "\n",
    "1. generate a value $\\hat{\\mathbf{x}}$ for $\\mathbf{x}$ from the distribution $p(\\mathbf{x})$ and\n",
    "1. find $p(z_k=1 | \\hat{\\mathbf{x}})$ for $k=1,\\ldots,K$ from the distribution $p(z_k=1 | \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM for Gaussian Mixtures\n",
    "\n",
    "Let $\\boldsymbol{\\pi} = (\\pi_1,\\ldots,\\pi_K)$, $\\boldsymbol{\\mu} = (\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K)$, \n",
    "$\\boldsymbol{\\Sigma} = (\\boldsymbol{\\Sigma}_1,\\ldots,\\boldsymbol{\\Sigma}_K)$, $\\mathbf{X} = {\\mathbf{x}_1,\\ldots,\\mathbf{x}_N}$, and $\\mathbf{Z} = {\\mathbf{z}_1,\\ldots,\\mathbf{z}_N}$, where $\\mathbf{z}_n$ is a latent variable corresponding to $\\mathbf{x}_n$.\n",
    "\n",
    "All vectors are assumed to be column vectors.\n",
    "\n",
    "\n",
    "The log of the likelihood function is \n",
    "$$ \\ln p(\\mathbf{X}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\ln \\prod_n p(\\mathbf{x}_n|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \n",
    "\\sum_n \\ln \\left[ \\sum_k \\pi_k N(\\mathbf{x}_n | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right]$$\n",
    "\n",
    "\n",
    "To find maximum likelihood solutions:\n",
    "\n",
    "1. Solve $\\partial_{\\boldsymbol{\\mu}_k} \\ln p(\\mathbf{X}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = 0$ and get $$\\boldsymbol{\\mu}_k = \\frac{\\sum_n \\gamma(z_{nk})\\mathbf{x}_n}{\\sum_n \\gamma(z_{nk})}$$ \n",
    "\n",
    "1. Solve $\\partial_{\\boldsymbol{\\Sigma}_k} \\ln p(\\mathbf{X}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = 0$ and get $$\\boldsymbol{\\Sigma}_k = \\frac{\\sum_n \\gamma(z_{nk})(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^T}{\\sum_n \\gamma(z_{nk})}$$\n",
    "\n",
    "1. Maximize $\\ln p(\\mathbf{X}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ w.r.t. $\\boldsymbol{\\pi}$ subject to $\\sum_k \\pi_k = 1$ using a Lagrange multiplier and get $$\\pi_k = \\frac{\\sum_n \\gamma(z_{nk})}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm for Gaussian Mixtures\n",
    "\n",
    "1. Initialize $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$.\n",
    "\n",
    "1. Repeat until convergence:\n",
    "\n",
    "    1. __E step__ Evaluate $\\gamma(z_{nk}) = \\displaystyle{\\frac{\\pi_k N(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j} \\pi_j N(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}}$. \n",
    "\n",
    "    1. __M step__ Update $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$ as follows: letting $N_k = \\sum_n \\gamma(z_{nk})$,\n",
    "\n",
    "        * $\\displaystyle{\\boldsymbol{\\mu}_k^{new} = \\frac{\\sum_n \\gamma(z_{nk})\\mathbf{x}_n}{N_k}}$\n",
    "    \n",
    "        * $\\displaystyle{\\boldsymbol{\\Sigma}_k^{new} = \\frac{\\sum_n \\gamma(z_{nk})(\\mathbf{x}_n-\\boldsymbol{\\mu}_k^{new})(\\mathbf{x}_n-\\boldsymbol{\\mu}_k^{new})^T}{N_k}}$\n",
    "    \n",
    "        * $\\displaystyle{\\pi_k^{new} = \\frac{N_k}{N}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General EM Algorithm\n",
    "\n",
    "Goal of the EM algorithm: Find maximum likelihood solutions for modeling having latent variables\n",
    "\n",
    "* $\\mathbf{X} = (\\mathbf{x}_1,\\ldots,\\mathbf{x}_N)$: the set of all observed data\n",
    "* $\\mathbf{Z} = (\\mathbf{z}_1,\\ldots,\\mathbf{z}_N)$: the set of all latent variables\n",
    "* $\\boldsymbol{\\theta}$: the set of all model parameters (such as $\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}$ in the Gaussian Mixtures)\n",
    "\n",
    "\n",
    "The log likelihood function $\\ln p(\\mathbf{X}|\\boldsymbol{\\theta}) = \\ln \\left[\\sum_{\\mathbf{Z}} p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})\\right]$ is of the form $\\ln (\\sum)$, resulting in complicated expressions for the maximum likelihood solution. \n",
    "\n",
    "Instead, we consider the expected value of the log likelihood $\\ln p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$ under the posterior distribution $p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta})$.\n",
    "\n",
    "In the E step, we evaluate the posterior distribution $p(\\mathbf{Z} | \\mathbf{X},\\boldsymbol{\\theta}^{old})$.\n",
    "\n",
    "In the M step, we maximize $\\boldsymbol{\\theta} \\mapsto \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{old})\\ln p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$.\n",
    "\n",
    "\n",
    "## EM Algorithm\n",
    "\n",
    "1. Initialize $\\boldsymbol{\\theta}^{old}$.\n",
    "\n",
    "1. Repeat until convergence:\n",
    "\n",
    "    1. __E step__: Evaluate $p(\\mathbf{Z}|\\mathbf{X}, \\boldsymbol{\\theta}^{old})$.\n",
    "    \n",
    "    1. __M step__: Evaluate $\\boldsymbol{\\theta}^{new}$ maximizing $\\boldsymbol{\\theta} \\mapsto \\sum_\\mathbf{Z} p(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{old})\\ln p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta})$ and update $\\boldsymbol{\\theta}^{old} \\leftarrow \\boldsymbol{\\theta}^{new}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
