{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "* Wikipedia\n",
    "\n",
    "* https://online.stat.psu.edu/stat510"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "\n",
    "## Stationary series\n",
    "\n",
    "A series $x_t$ is said to be (weakly) stationary if $E[x_t]$, $V[X_t]$, and $\\text{Cov}(x_t, x_{t-1})$ do not depend on $t$.\n",
    "\n",
    "## Differencing\n",
    "\n",
    "* $B$ = the backshift operator\n",
    "\n",
    "    For example, $B x_t = x_{t-1}$ and $B^k x_t = x_{t-k}$.\n",
    "    \n",
    "    \n",
    "\n",
    "* $\\nabla = 1 - B$ = the first difference\n",
    "\n",
    "    For example, $\\nabla x_t = x_t - x_{t-1}$ and $\\nabla^2 x_t = (1-B)^2 x_t = (1-2B+B^2) x_t = x_t -2 x_{t-1} + x_{t-2}$.\n",
    "    \n",
    "    \n",
    "\n",
    "## White Noise\n",
    "\n",
    "A time series is white noise if the variables are independent and identically distributed with a mean of zero.\n",
    "\n",
    "If the variance changes over time or if values correlate with lag values, the time series is not white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Decomposition\n",
    "\n",
    "* Additive model: $y(t) = \\text{Level} + \\text{Trend} + \\text{Seasonality} + \\text{Noise} $\n",
    "\n",
    "\n",
    "* Multiplicative model: $y(t) = \\text{Level} \\times \\text{Trend}  \\times \\text{Seasonality}  \\times \\text{Noise} $\n",
    "\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(x, model='additive')\n",
    "# x is a pandas object with a timeseries index with a freq not set to None.\n",
    "# If x is not a pandas object, freq= should be given in seasonal_decompose().\n",
    "\n",
    "result.plot()\n",
    "\n",
    "# We can get the following data:\n",
    "result.observed\n",
    "result.trend\n",
    "result.reasonal\n",
    "result.resid\n",
    "result.nobs\n",
    "```\n",
    "\n",
    "\n",
    "## Detrending\n",
    "\n",
    "* By differencing: $\\nabla x_t = x_t - x_{t-1}$, more generally, $\\nabla^d x_t = (1-B)^d x_t$, where $B$ is the backshift operator. \n",
    "\n",
    "Example. For a random walk model given as $x_t = x_{t-1} + w_t$ with $w_t\\sim N(0,\\sigma^2)$, we have $\\nabla x_t = x_t - x_{t-1} = w_t$.\n",
    "\n",
    "* By fitting a model: If a model's prediction of $x_t$ is $\\hat{x}_t$, $x_t - \\hat{x}_t$ may detrend the time series.\n",
    "\n",
    "\n",
    "## Deseasonalizing\n",
    "\n",
    "* By differencing: $x_t - x_{t-p}$, where $p$ is a constant depending on the seasonal length\n",
    "\n",
    "    We may resample the time series for a stable result. For example, if $x$ is a temperature dataset measured daily, we can apply the above differencing method with $p=12$ after making a monthly dataset: \n",
    "    ```\n",
    "    x = x.resample('M').mean()\n",
    "    ```\n",
    "    \n",
    "* By fitting a model: If a model's prediction of $x_t$ is $\\hat{x}_t$, $x_t - \\hat{x}_t$ may deseasonalize the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation, Partial Autocorrelation\n",
    "\n",
    "\n",
    "## Autocorrelation\n",
    "\n",
    "For an ACF (AutoCorrelation Function) to make sense, the series must be a weakly stationary series. \n",
    "\n",
    "$\\text{ACF}(x_t, x_{t-h}) = \\displaystyle{\\frac{\\text{Cov}(x_t, x_{t-h})}{V[x_t]}}$\n",
    "\n",
    "\n",
    "In the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order $q$, we have $R(\\tau )\\neq 0$ for $\\tau =0,1,\\ldots ,q$ and $R(\\tau )=0$ for $\\tau > q$.\n",
    "\n",
    "\n",
    "\n",
    "* statsmodels.graphics.tsaplots.plot_acf()\n",
    "\n",
    "By default, this is set to a 95% confidence interval.\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "dta = sm.datasets.sunspots.load_pandas().data\n",
    "sm.graphics.tsa.plot_acf(dta.values[:,1], lags=40);\n",
    "```\n",
    "<img src=\"images/image_01.png\" style=\"width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas.plotting.autocorrelation_plot()\n",
    "\n",
    "    autocorrelation_plot(series, ax, ...): Autocorrelation plot for time series (autocorrelations over lags)\n",
    "\n",
    "    x-axis: lag numbers\n",
    "\n",
    "    y-axis: correlation coefficients\n",
    "\n",
    "    solid lines, dashed lines: 95% and 99% confidence interval for the correlation values, resp\n",
    "\n",
    "```python\n",
    "# dta: the data used above\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(dta.values[:,1])\n",
    "```\n",
    "<img src=\"images/image_06.png\" style=\"width: 300px; float: left;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Autocorrelation\n",
    "\n",
    "Given a time series $z_{t}$, the partial autocorrelation of lag $k$ is the autocorrelation between $z_{t}$ and $z_{t+k}$ that is not accounted for by lags 1 through $k-1$. \n",
    "\n",
    "When plotting the partial autocorrelative functions one could determine the appropriate lags $p$ in an AR(p) model or in an ARIMA (p,d,q) model.\n",
    "\n",
    "\n",
    "* statsmodels.graphics.tsaplots.plot_pacf()\n",
    "\n",
    "```python\n",
    "# dta: the data used above\n",
    "sm.graphics.tsa.plot_pacf(dta.values[:,1], lags=40);\n",
    "```\n",
    "\n",
    "<img src=\"images/image_02.png\" style=\"width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying a model using ACF, PACF\n",
    "\n",
    "* AR model: PACF\n",
    "\n",
    "\n",
    "* MA model: ACF\n",
    "\n",
    "\n",
    "* If the ACF and PACF do not tail off, the series is non-stationary and differencing will be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "\n",
    "## Random walk\n",
    "\n",
    "Let $Z$ be a random variable with $P(Z=1) = P(Z=-1) = 0.5$.\n",
    "\n",
    "${X_n}$ is the simple random walk if\n",
    "\n",
    "1. $X_0 = 0$.\n",
    "\n",
    "2. $X_n = X_{n-1} + Z$ for $n=1,2,\\ldots$.\n",
    "\n",
    "\n",
    "Equivalently, $X_n = \\sum_{i=1}^n Z_i$, where $Z_i$ are independent random variables, where each $Z_i$ is either 1 or -1 with a 50% probability for either value.\n",
    "\n",
    "We can show $E[X_n]=0$ and $V[X_n] = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRegressive (AR) model\n",
    "\n",
    "AR(p) = an autoregressive model of order p\n",
    "\n",
    "$X_t = \\delta + \\varphi_1 X_{t-1} + \\cdots + \\varphi_p X_{t-p} + \\epsilon_t = \\delta + (\\varphi_1 B + \\cdots + \\varphi_p B^p)X_t + \\epsilon_t$, where $B$ is the backshift operator. \n",
    "\n",
    "Equivalently, $\\phi[B]X_t = c + \\epsilon_t$, where $\\phi[B] = 1 - \\varphi_1 B - \\cdots -\\varphi_p B^p$.\n",
    "\n",
    "We assume that the series $X_1, X_2, \\ldots$ is (weakly) stationary. \n",
    "\n",
    "* For AR(1), the correlation between observations $h$ time perios apart is $\\varphi_1^h$. A requirement for a stationary AR(1) is $|\\varphi_1|<1$.\n",
    "\n",
    "\n",
    "\n",
    "* For an AR(p) model, $\\text{PACF}(h) \\neq 0$ (statistically significant) for $h\\leq p$ and $\\text{PACF}(h) = 0$ (not statistically significant) for $h>p$.\n",
    "\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "# Here train and test are 1d numpy arrays.\n",
    "\n",
    "model = AR(train)\n",
    "model_fitted = model.fit()\n",
    "\n",
    "model_fitted.k_ar                  # lag length = the order of the AR = p\n",
    "model_fitted.params                # the (p+1) fitted parameters of the model\n",
    "\n",
    "preds = model_fitted.predict(start=len(train), end=len(train) + len(test)-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average (MA) model\n",
    "\n",
    "MA(q) = an moving average model of order q\n",
    "\n",
    "$X_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots +\\theta_q \\epsilon_{t-q} = \\mu + (1 + \\theta_1 B + \\cdots +\\theta_q B^q) \\epsilon_t$, where $B$ is the backshift operator. \n",
    "\n",
    "* Contrary to the AR model, the finite MA model is always stationary.\n",
    "\n",
    "\n",
    "* For an MA(q) model $\\text{ACF}(h) \\neq 0$ (statistically significant) for $h\\leq q$ and $\\text{ACF}(h) = 0$ (not statistically significant) for $h>q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRegressive Moving Average (ARMA) model\n",
    "\n",
    "ARMA(p,q) = an model with p autoregressive terms and q moving-average terms\n",
    "\n",
    "$X_t = \\delta + \\epsilon_t  + (\\varphi_1 B + \\cdots + \\varphi_p B^p)X_t + (\\theta_1 B + \\cdots +\\theta_q B^q) \\epsilon_t$\n",
    "\n",
    "The error terms $\\epsilon_t$ are assumed to be i.i.d. sampled from $N(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRegressive Integrated Moving Average (ARIMA) model\n",
    "\n",
    "ARIMA models are also called Box-Jenkins models that include autoregressive terms, moving average terms, and differencing operations. \n",
    "\n",
    "\n",
    "ARMA(p,q) can be written as\n",
    "\n",
    "$\\left( 1-\\sum_{i=1}^p \\varphi_i B^i \\right) X_t = \\delta + \\left( 1 + \\sum_{i=1}^q \\theta_i B^i\\right) \\epsilon_t$.\n",
    "\n",
    "ARIMA(p,d,q) is of the form\n",
    "\n",
    "$\\left( 1-\\sum_{i=1}^p \\varphi_i B^i \\right) \\nabla^d X_t = \\delta + \\left( 1 + \\sum_{i=1}^q \\theta_i B^i\\right) \\epsilon_t$, where $\\nabla = 1 - B$ is the first difference operator. \n",
    "\n",
    "(Detrend a given time series $X_t$ by $\\nabla^d X_t$ and apply an ARMA model to the detrended time series)\n",
    "\n",
    "p: the number of lag observations (the lag order)\n",
    "\n",
    "d: the number of times that the raw observations are differenced (the degree of differencing)\n",
    "\n",
    "q: the size of the moving average window (the order of moving average)\n",
    "\n",
    "\n",
    "* Any ARIMA model can be converted to an MA($\\infty$). \n",
    "\n",
    "\n",
    "* Any finite order MA is an AR($\\infty$) and any finite order AR is an infinite order MA($\\infty$).\n",
    "\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Here x is a 1d numpy array.\n",
    "model = ARIMA(x, order=(p=5,d=1,q=2))\n",
    "model_fitted = model.fit()\n",
    "\n",
    "model_fitted.forecast()      # forecast(steps=1,..)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box - Jenkins method\n",
    "\n",
    "The Box–Jenkins method applies ARMA or ARIMA models to find the best fit of a time-series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dickey–Fuller test\n",
    "\n",
    "The Dickey–Fuller test tests the null hypothesis that a unit root is present in an autoregressive model.\n",
    "\n",
    "AR(1) model is $x_t = \\rho x_{t-1} + \\epsilon_t$. A unit root is present if $\\rho=1$. \n",
    "\n",
    "The model can be written as $\\Delta x_t = \\gamma x_{t-1} + \\epsilon_t$, where $\\gamma = \\rho -1$. \n",
    "\n",
    "Since the test is done over the residual term rather than raw data, it is not possible to use standard t-distribution to provide critical values. This statistic $t$ has a specific distribution known as the Dickey–Fuller table.\n",
    "\n",
    "There are three main versions of the test:\n",
    "\n",
    "1. Test for a unit root: $\\Delta x_t = \\gamma x_{t-1} + \\epsilon_t$\n",
    "1. Test for a unit root with drift: $\\Delta x_t = \\alpha + \\gamma x_{t-1} + \\epsilon_t$\n",
    "1. Test for a unit root with drift and deterministic time trend: $\\Delta x_t = \\alpha + \\beta t + \\gamma x_{t-1} + \\epsilon_t$\n",
    "\n",
    "\n",
    "## Augmented Dickey–Fuller test\n",
    "\n",
    "The testing procedure for the ADF test is the same as for the Dickey–Fuller test but it is applied to the model\n",
    "\n",
    "$\\Delta x_t = \\alpha + \\beta t + \\gamma x_{t-1} + \\delta_1\\Delta x_{t-1} + \\cdots + \\delta_{p-1} \\Delta x_{t-p+1} + \\epsilon_t$\n",
    "\n",
    "The ADF statistic is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# random_walk is a list of numbers.\n",
    "result = adfuller(random_walk)\n",
    "\n",
    "result[0]      # ADF statistic\n",
    "result[1]      # p-value\n",
    "result[4]      # Critical values for the test statistic at the 1 %, 5 %, and 10 % levels. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions in Time series\n",
    "\n",
    "\n",
    "\n",
    "## pandas.plotting.lag_plot()\n",
    "\n",
    "lag_plot(series, lag, ...): Lag plot for time series.\n",
    "\n",
    "It show the scatter plot of the points $(y_t, y_{t+\\text{lag}})$.\n",
    "\n",
    "\n",
    "## sklearn.model_selection.TimeSeriesSplit()\n",
    "\n",
    "`TimeSeriesSplit()` in `sklearn.model_selection` provides train/test indices to split time series data samples. This cross-validation object is a variation of KFold. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# X is a 2D numpy array with 3252 rows\n",
    "\n",
    "splits = TimeSeriesSplit(n_splits=3)\n",
    "for train_idx, test_idx in splits.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    # ...\n",
    "    print((train_idx[0], train_idx[-1], len(train_idx)), (test_idx[0], test_idx[-1], len(test_idx)))\n",
    "\n",
    "(0, 812, 813) (813, 1625, 813)\n",
    "(0, 1625, 1626) (1626, 2438, 813)\n",
    "(0, 2438, 2439) (2439, 3251, 813)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
