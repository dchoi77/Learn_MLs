{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "\n",
    "* Data becomes sparse and should grow exponentially to maintain an accurate analysis as dimension increases.\n",
    "\n",
    "\n",
    "* Data that are close to each other in a low dimensional space may be far away in higher dimensional spaces, making predictions much less reliable. The more features the training set has, the greater the risk of overfitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "* PCA finds the axis having the largest amount of variance in the training dataset. \n",
    "\n",
    "\n",
    "* PCA assumes that the dataset is centered around the origin. \n",
    "\n",
    "\n",
    "* Given the training set matrix $X\\in M_{m,n}$,\n",
    "\n",
    "    1. computer the zero-centered matrix $X_0 = X - np.mean(X,0)$;  \n",
    "    1. apply the SVD to $X_0$ and get $X_0 = U\\Sigma V^T$, where \n",
    "    \n",
    "        * $U\\in M_{m,m}$ and $V\\in M_{n,n}$ are unitary and\n",
    "        * $\\Sigma\\in M_{m,n}$ is a rectangular diagonal matrix whose diagonal entries are nonnegative real numbers and arranged in descending order.\n",
    "    \n",
    "    \n",
    "* The columns of $V^*$ are called the __principal components__. The diagonal entries of $\\Sigma$ are called the __singular values__.\n",
    "\n",
    "\n",
    "* Note $X_0V = U\\Sigma$. If we want to project the training instances from $\\mathbb{R}^n$ onto $\\mathbb{R}^k\\;(k\\leq n)$, we compute $U_k\\Sigma_k$, where $U_k = U[:,:k]\\in M_{n,k}$ and $\\Sigma_k = \\Sigma[:k,:k]\\in M_{k,k}$. Equivalently, we can compute $X_0 V_k$, where $V_k = V[:,:k]\\in M_{n,k}$.\n",
    "\n",
    "\n",
    "* Conversely, to reconstruct a dataset ($\\in M_{r,n}$) from $Y\\in M_{r,k}$, we compute $Y V_k^T$ and then add $np.mean(X,0)$ to it.\n",
    "\n",
    "\n",
    "* If $s_1,\\ldots,s_k$ are the first $k$ singular values of $X_0$ (_i.e._ the first $k$ diagonal entries of $\\Sigma$), then the __explained variance__ is $\\frac{1}{m-1}[s_1^2,\\ldots,s_k^2]$ and the __explained variance ratio__ is $\\frac{1}{s_1^2+\\cdots+s_k^2}[s_1^2,\\ldots,s_k^2]$.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "clf = PCA(n_components, svd_solver, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: scikit-learn.org\n",
    "\n",
    "\n",
    "## Incremental PCA\n",
    "\n",
    "This algorithm has constant memory complexity, enabling use of np.memmap files without loading the entire file into memory, and allows sparse input.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "clf = IncrementalPCA(n_components, ...)\n",
    "```\n",
    "\n",
    "\n",
    "## Randomized PCA\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "clf = PCA(n_components, svd_solver='randomized', ...)\n",
    "```\n",
    "\n",
    "\n",
    "## Kernel PCA\n",
    "\n",
    "Apply the kernel trick to PCA.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import KernelPCA\n",
    "clf = KernelPCA(n_components, kernel,...)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
