{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "* Let $p_k$ be the ratio of class $k$ instances among the training instances in a given node.\n",
    "\n",
    "    * The __Gini__ score of the node is $\\sum_{k=1}^{n} (1-p_k) p_k$ or equivalently $1 - \\sum_{k=1}^{n} p_k^2$. Assume there are three class labels. If a node contains 30 instances for class 1, 20 instances for class 2, and 50 instances for class 3, then the Gini score of the node is $1 - (30/100)^2 - (20/100)^2 - (50/100)^2 = 0.62$. If another node contains 50 instances for class 1 and 0 instances for other classes, then its Gini score is $1 - (50/50)^2 - (0/50)^2 - (0/50)^2 = 0$.\n",
    "    \n",
    "    * The __Entropy__ score of the node is $ - \\sum_{k=1}^{n} p_k \\log{p_k}$.\n",
    " \n",
    " \n",
    "* Suppose node $D$ has $m$ childlren nodes $D_1,\\ldots,D_m$. Let $I(D)$ denote the impurity of node $D$ such as Gini or Entropy and $|D|$ be the number of instances in node $D$. The cost at node $D$ is $\\sum_{j=1}^{m} \\frac{|D_j|}{|D|} I(D_j)$ and the __information again__ at $D$ is $I(D) - \\sum_{j=1}^{m} \\frac{|D_j|}{|D|} I(D_j)$.\n",
    "\n",
    "* In a regression tree, the cost at node $D$ is $\\sum_{j=1}^{m} \\frac{|D_j|}{|D|} MSE(D_j)$, where $MSE(D)$ denotes the Mean Square Error of the instances in node $D$; that is, $MSE(D) = \\sum_{i\\in D} (\\hat{y} - y_i)^2$, where $\\hat{y} = \\frac{1}{|D|}\\sum_{i\\in D} y_i$.\n",
    "\n",
    "\n",
    "* To compute the probability that an instance belongs to class $k$, find the leaf node containing the instance and compute the ratio of the training instances of class k in the node. For example, if there are three classes and the leaf node corresponding to a given instance contains 48 instances for class 1, 3 instances for class 2, and 5 instances for class 3, then the probabilities that the instance belongs to each class are 48/56, 3/56, and 5/56, respectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "\n",
    "## Bagging (Bootstrap AGGregatING))\n",
    "\n",
    "References: https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Given a standard training set $D$ of size $n$, bagging generates $m$ new training sets $D_{i}$, each of size $p$, by sampling from $D$ uniformly and with replacement. \n",
    "\n",
    "If $p=n$, this kind of sample is known as a __bootstrap sample__. Then, $m$ models are fitted using the $m$ bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(base_estimator, n_estimators, max_samples, max_features, bootstrap, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* Random Forests = Sample Bagging + Feature Bagging \n",
    "\n",
    "\n",
    "* At any split in each decision tree, a sample of $m$ features is chosen randomly from the full set of all features. Typically, $m$ is about the square root of the total number of features.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators, max_features, max_samples, bootstrap, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble algorithm that combines several weak learners sequentially into a strong learner.\n",
    "\n",
    "\n",
    "### AdaBoost (ADAptive Boosting)\n",
    "\n",
    "* Initially, each instance weight $w_i$ is set to $1/n$, where $n$ is the number of instances.\n",
    "\n",
    "* At the $j$-th iteration, we use a (weak) learner $M_j$ to update the weights $w_1,\\ldots,w_n$ and compute the learner's weight $\\alpha_j$ as follows:\n",
    "\n",
    "    1. Train the $j$-th learner on the training set with weights $w_1,\\ldots,w_n$.\n",
    "    1. Compute the total error rate $r_j$ defined by the sum of all $w_i$s corresponding to misclassification results.\n",
    "    1. Compute the learner's weights $\\alpha_j$ defined by $\\eta \\log\\frac{1-r_j}{r_j}$, where $\\eta$ is a hyperparameter controlling the learning rate.\n",
    "    1. If $w_i$ is the weight of a misclassified instance, update it by $w_i \\exp(\\alpha_j)$ or equivalently by $w_i \\left(\\frac{1-r_j}{r_j}\\right)^\\eta$.\n",
    "    1. Normalize all weights; that is, divide each weight $w_i$ by $\\sum_{i=1}^{n} w_i$ so that the sum of weights is 1.\n",
    "\n",
    "\n",
    "* Prediction: Suppose we used five weak learners $M_1,\\ldots,M_5$ in the algorithm and there are 3 class labels. Given an instance $\\mathbf{x}$, assume $M_1(\\mathbf{x}) = 3$, $M_2(\\mathbf{x}) = 2$, $M_3(\\mathbf{x}) = 3$, $M_4(\\mathbf{x}) = 1$, $M_5(\\mathbf{x}) = 3$. Then the model weights are $\\alpha_4$ for class 1, $\\alpha_2$ for class 2, and $\\alpha_1 + \\alpha_3 + \\alpha_5$ for class 3. The algorithm's prediction is the class label corresponding to the highest model weights.   \n",
    "\n",
    "\n",
    "* AdaBoostClassifier in Scikit-Learn uses a multiclass version of AdaBoost:\n",
    "    \n",
    "    * SAMME for discrete boosting algorithm\n",
    "    * SAMME.R for real boosting algorithm (if the base estimator can compute class probabilities)\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator, n_estimators, learning_rate, algorithm, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "\n",
    "Let $I(D)$ be the impurity of node $D$ such as Gini or Entropy. The importance of node $D$ in a decision tree is computed as follows:\n",
    "\n",
    "$$NI(D) = |D| I(D) - |D_{left}| I(D_{left}) - |D_{right}| I(D_{right})$$\n",
    "\n",
    "\n",
    "The importance of feature $i$ on a decision tree consisting of nodes $D_1,\\ldots,D_m$ is computed as follows:\n",
    "\n",
    "$$FI(i) = \\frac{\\sum \\{NI(D_j): D_j \\text{ splits on feature } i\\}}{\\sum_{j=1}^{m} NI(D_j)}$$\n",
    "\n",
    "\n",
    "The feature importances can be normalized to\n",
    "\n",
    "$$NFI(i) = \\frac{FI(i)}{\\sum_{k\\in \\text{all features}} FI(k)}$$\n",
    "\n",
    "\n",
    "The importance of feature $i$ on a random forest consisting of $N$ decision trees is the sum of $NFI(i)$ of the decision trees divided by $N$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
