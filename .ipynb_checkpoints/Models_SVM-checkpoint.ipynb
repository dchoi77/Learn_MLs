{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## Maximal Margin Classifier\n",
    "\n",
    "* Decision boundary: $f(\\mathbf{x}) = \\beta_0 + \\boldsymbol{\\beta}^T \\mathbf{x} = 0$\n",
    "\n",
    "\n",
    "* Distance between the decision boundary and a point $\\mathbf{x}_0$ : $ |f(\\mathbf{x}_0)|/ ||\\boldsymbol{\\beta}||$\n",
    "\n",
    "\n",
    "* Given $\\mathbf{x}_0$, its corresponding label $y_0$ is the sign of $f(\\mathbf{x}_0)$. That is, if $f(\\mathbf{x}_0) > 0$, $y_0 = 1$ and if $f(\\mathbf{x}_0) < 0$, $y_0 = -1$.\n",
    "\n",
    "\n",
    "* Using the labels, the distance between the decision boundary and a point $\\mathbf{x}_0$ can be written $ y_0 f(\\mathbf{x}_0)/ ||\\boldsymbol{\\beta}||$.\n",
    "\n",
    "\n",
    "* If the marginal boundaries are given by $|f(\\mathbf{x})| = 1$, the maximal margin classifier maximizes $1 / ||\\boldsymbol{\\beta}||$ subject to the constraints $y_i f(\\mathbf{x}_i) \\geq 1$ for all $\\mathbf{x}_i$ in the training set; equivalently, it minimizes $\\frac{1}{2} ||\\boldsymbol{\\beta}||^2$ subject to $y_i f(\\mathbf{x}_i) \\geq 1$ for all $i$.\n",
    "\n",
    "\n",
    "* Note that the distance between the decision boundary ($f(\\mathbf{x})=0$) and the marginal boundaries ($f(\\mathbf{x})=\\pm 1$) is $1/||\\boldsymbol{\\beta}||$.\n",
    "\n",
    "\n",
    "* If $y_i f(\\mathbf{x}_i) \\geq 1$, then the loss is zero. Meanwhile, if $y_i f(\\mathbf{x}_i) \\leq 1$, then the loss is $1 - y_i f(\\mathbf{x}_i)$. Thus, the loss can be written $L(f(\\mathbf{x}_i))$, where $L(t)$ is the hinge loss function defined by $L(t)=0$ for $t\\geq 1$ and $L(t)=1-t$ for $t < 1$. \n",
    "\n",
    "\n",
    "* After the training is done, $y_i f(\\mathbf{x}_i) \\geq 1$ for all $i$. $\\mathbf{x}_i$ is called a __support vector__, if $y_i f(\\mathbf{x}_i) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "<img src=\"images/svc.png\" style=\"width: 800px;\">\n",
    "\n",
    "* SVC introduces variables $\\epsilon_i \\geq 0$ called __slack variables__. Thus, if the training set has $N$ datapoints, then SVC has $2N+1$ variables ($N$ for $\\boldsymbol{\\beta}$, 1 for $\\beta_0$, and $N$ for the slack variables.)\n",
    "\n",
    "\n",
    "* If we assume $||\\boldsymbol{\\beta}|| = 1$ and $c > 0$ is a hyperparameter, then SVC maximizes $M$ subject to $y_i f(\\mathbf{x}_i) \\geq M (1-\\epsilon_i)$ for all $i$, $\\epsilon_i\\geq 0$ for all $i$, and $\\sum \\epsilon_i \\leq c$.\n",
    "\n",
    "\n",
    "* If $\\epsilon_i > 1$, then the classifier misclassifies the class label for $\\mathbf{x}_i$. Thus the total number of misclassifications is bounded by $\\sum \\epsilon_i$ which is controlled by the hyperparameter $c$.\n",
    "\n",
    "\n",
    "* In scikit-learn, the function SVC() uses a hyperparameter C which is essentially $1/c$. \n",
    "    * Large C $\\iff$ Small margin $\\iff$ High variance $\\iff$ Large complexity\n",
    "    * Small C $\\iff$ Large margin $\\iff$ Low variance $\\iff$ Small complexity\n",
    "    * May assume that C stands for Complexity.\n",
    "    \n",
    "    \n",
    "* Equivalently, SVC minimizes $\\frac{1}{2} ||\\boldsymbol{\\beta}||^2 + C \\sum \\epsilon_i$ subject to $y_i f(\\mathbf{x}_i) \\geq 1-\\epsilon_i$ and $\\epsilon_i \\geq 0$ for all $i$. If $y_i f(\\mathbf{x}_i) \\leq 1$, then $x_i$ is a __support vector__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
